

<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>phessianfree’s documentation &mdash; phessianfree 0.1 documentation</title>
    
    <link rel="stylesheet" href="_static/default.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '',
        VERSION:     '0.1',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <link rel="top" title="phessianfree 0.1 documentation" href="#" />
    <link rel="next" title="Optimization example overview" href="examples.html" /> 
  </head>
  <body>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="examples.html" title="Optimization example overview"
             accesskey="N">next</a> |</li>
        <li><a href="#">phessianfree 0.1 documentation</a> &raquo;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body">
            
  <div class="section" id="phessianfree-s-documentation">
<h1>phessianfree&#8217;s documentation<a class="headerlink" href="#phessianfree-s-documentation" title="Permalink to this headline">¶</a></h1>
<p>phessianfree is a modern optimization method for smooth, unconstained minimization problems, intended to be used in place of LBFGS. It is designed to take advantage of the structure of regularized loss minimization problems, where the objective decomposes as a sum over a large number of terms. By taking advantage of this structure, it is able to achieve rapid initial convergence similar to stochastic gradient descent methods, but without sacrificing the later stage convergence properties of batch methods such as LBFGS.</p>
<p>phessianfree uses a newton-cg method, somestimes called a hessian-free method, where the search direction at each step is improved using a linear solver, to bring it closer to the ideal newton step. Hessian-vector products are evaluated without forming the actual hessian, using finite difference methods, with an adjustable overhead, defaulting to 10% more computation per iteration than standard lbfgs methods.</p>
<p>See the example code for comparisons between LBFGS, phessianfree and SAG on training a standard classifier, where pnewton is able to converge in test loss up to 4 times faster than LBFGS. Example code with MNIST data bundled is available on <a class="reference external" href="https://github.com/adefazio/phessianfree">github</a>.</p>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="examples.html">Optimization example overview</a></li>
</ul>
</div>
<div class="section" id="core-method">
<h2>Core method<a class="headerlink" href="#core-method" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="phessianfree.optimize">
<tt class="descclassname">phessianfree.</tt><tt class="descname">optimize</tt><big>(</big><em>f</em>, <em>x0</em>, <em>ndata</em>, <em>gtol=1e-05</em>, <em>maxiter=100</em>, <em>callback=None</em>, <em>props={}</em><big>)</big><a class="headerlink" href="#phessianfree.optimize" title="Permalink to this definition">¶</a></dt>
<dd><p>This method can be invoked in a simlar way as lbfgs routines in Scipy,
with the following differences:</p>
<blockquote>
<div><ul class="simple">
<li>f takes additional arguments &#8216;s&#8217; and &#8216;e&#8217; that signify a range of 
points to evaluate the objective over.</li>
<li>The callback gives additional information</li>
<li>logging is performed using the standard python logging framework</li>
</ul>
</div></blockquote>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>f</strong> (<em>function</em>) &#8211; Objective function, taking arguments (x,s,e), where
(s,e) is the range of datapoints over which to evaluate
the objective.</li>
<li><strong>x0</strong> (<em>vector</em>) &#8211; Initial point</li>
<li><strong>ndata</strong> (<em>int</em>) &#8211; Number of points in dataset. The passed function 
will be invoked with s,e between 0 and ndata.</li>
<li><strong>gtol</strong> (<em>float</em>) &#8211; stopping criterion, measured in 2-norm.</li>
<li><strong>maxiter</strong> (<em>int</em>) &#8211; Maximum number of steps to complete. Note that this does
not count line search iterations.</li>
<li><strong>callback</strong> (<em>function</em>) &#8211; Invoked with (xk, fval, gfk, pointsProcessed), 
useful for tracking progress for latter plotting. 
PlottingCallback in the convergence module can do
this for you.</li>
<li><strong>props</strong> (<em>object</em>) &#8211; <dl class="docutils">
<dt>Map of additional parameters:</dt>
<dd><ul class="first last">
<li><dl class="first docutils">
<dt><strong>parts</strong> (<em>integer</em> default 100)</dt>
<dd>For computing gradients and hessian vector products,
the data is split into this many parts. Calls to your
objective function will be in roughly ndata/parts.
The default of 100 is suitable for most datasets, 
smaller numbers are only useful if the dataset is small
or non-homogeneous, in which case the hessian free method
is ineffective. Larger numbers of parts may improve 
convergence, but result proportionally more internal overhead.</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><strong>subsetVariant</strong> (<em>string</em> default &#8216;lbfgs&#8217;)</dt>
<dd>Setting this to &#8216;cg&#8217; gives the standard conjugate gradient method
for solving the linear system Hp = -g, to find the search direction
p from the gradient g and hessian H. This is computed over only one
of the parts, so only a small amount of the data is seen.
Setting this to &#8216;lbfgs&#8217; uses a stochastic minibatch lbfgs method
for solving the linear subproblem. This sees many more parts of the
data, but is only able to make half as many steps for the same 
computation time. For problems without extreme curvature, lbfgs
works much better than cg. If the condition number of the hessian
is very large however, cg is the better option. In those cases
the solveFraction property should normally be increases as well.</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><strong>solveFraction</strong> (<em>float</em> default 0.2)</dt>
<dd>The cg or lbfgs linear solvers perform a number of iterations
such that <strong>solveFraction</strong> fraction of overhead is incurred.
For example, if set to 0.2 and 100 parts, 20 cg iterations on 1
part will be preformed, if the cg subset variant is used.
If subsetObjective is off, then essentially 20% extra computation
is done per outer step over a standard lbfgs method (excluding line
searches).</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><strong>subsetObjective</strong> (<em>boolean</em> default True) </dt>
<dd>Turn on or off the use of subsets of data for 
computing gradients. If off, gradients are computed using 
the full dataset, but hessian-vector products still use subsets.
The size of the subset used for computing the gradient is adaptive
using bounds on the approximation error.</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><strong>gradRelErrorBound</strong> (<em>float</em> default 0.1)</dt>
<dd>At a search point, the gradient is computed over enough parts
so that the relative variance of the gradients is brought below
this threshold. 0.1 is conservative; better results may be 
achieved by using values up to about 0.4. Larger values may cause
erratic convergence behavior though.</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><strong>lbfgsMemory</strong> (<em>integer</em> 10)</dt>
<dd>The lbfgs search direction is used as the initial guess at the 
search direction for the cg and lbfgs inner solves. This controls
the memory used for that. The same memory is used for the inner 
lbfgs solve. Changing this has less of an effect than it would
on a standard lbfgs implementation.</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><strong>fdEps</strong> (<em>float</em> default 1e-8)</dt>
<dd>Unless a gaussNewtonProd method is implemented, hessian vector
products are computed by using finite differences. Unlike 
applying finite differences to approximate the gradient, the FD
method allows for the computation of hessian-vector products
at the cost of only one subset gradient evaluation.
If convergence plots become erratic near the optimum, tuning this
parameter can help. This normally occurs long after the test loss
has plateaued however.</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><strong>innerSolveAverage</strong> (<em>boolean</em> default False)</dt>
<dd>Applicable only if subsetVariant is lbfgs, this turns on the 
use of 50% sequence suffix averaging for the inner solve.
If a large number of parts (say 1000) is being used, this
can give better results.</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><strong>innerSolveStepFactor</strong> (<em>float</em> default 0.5)</dt>
<dd>The lbfgs subsetVariant is stochastic, however it uses the 
fact that quadratic problems have a simple formula for exact line
searches, in order to make better step choices than simple SGD.
Doing an exact line search makes overconfident steps however, and
so the step is scaled by this factor. If the lbfgs linear solve
is diverging, decrease this.</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">(xk, fval)</p>
</td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">If your objective is non-convex, you need to explictly provide a 
function that computes matrix vector products against the 
Gauss-Newton approximation to the hessian. You can do this
by making <strong>f</strong> an object with a __call__ method that implements the 
objective function as above, and a gaussNewtonProd(x, v, s, e) 
method that implements the matrix vector product against v for
the GN approximation at x over the datapoints (s,e). This is
illustrated in the autoencoder example code.</p>
</div>
</dd></dl>

</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar">
        <div class="sphinxsidebarwrapper">
  <h3><a href="#">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">phessianfree&#8217;s documentation</a><ul>
<li><a class="reference internal" href="#core-method">Core method</a></li>
</ul>
</li>
</ul>

  <h4>Next topic</h4>
  <p class="topless"><a href="examples.html"
                        title="next chapter">Optimization example overview</a></p>
  <h3>This Page</h3>
  <ul class="this-page-menu">
    <li><a href="_sources/index.txt"
           rel="nofollow">Show Source</a></li>
  </ul>
<div id="searchbox" style="display: none">
  <h3>Quick search</h3>
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
    </p>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="examples.html" title="Optimization example overview"
             >next</a> |</li>
        <li><a href="#">phessianfree 0.1 documentation</a> &raquo;</li> 
      </ul>
    </div>
    <div class="footer">
        &copy; Copyright 2012, Aaron Defazio.
      Created using <a href="http://sphinx.pocoo.org/">Sphinx</a> 1.1.3.
    </div>
  </body>
</html>